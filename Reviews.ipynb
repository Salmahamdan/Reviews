{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOuWZ8IQdTdt"
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "tfds.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0pJ0hoDfrbx"
   },
   "source": [
    "# Read Dataset\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Yt33DHBF48G"
   },
   "outputs": [],
   "source": [
    "dataset,info=tfds.load('imdb_reviews',with_info=True, as_supervised=True)\n",
    "train_dataset,test_dataset=dataset['train'],dataset['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W_MlwMJ6TiKx"
   },
   "outputs": [],
   "source": [
    "# Convert a subset of the training dataset to a Pandas DataFrame\n",
    "df = pd.DataFrame(columns=['text', 'label'])\n",
    "for text, label in train_dataset.take(3000):\n",
    "    df = df.append({'text': text.numpy().decode('utf-8'), 'label': label.numpy()}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OUHXjJ7uUxst"
   },
   "outputs": [],
   "source": [
    "#Overview of data , Showing the columns name and data type of each column\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pTYFViBwTppz"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mZzRgqnIU8LA"
   },
   "outputs": [],
   "source": [
    "#check the dataset shape (rows,columns)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "465xcL67gCfk"
   },
   "source": [
    "#Check for missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pZL3iiwfGZ1k"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "haFjb-_mgIw1"
   },
   "source": [
    "# Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-yzwMmP0fZvh"
   },
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "75PhO_Vzgdto"
   },
   "outputs": [],
   "source": [
    "df.describe(include=\"O\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CIqREHYSgfXw"
   },
   "source": [
    "#Label Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i355QJLcxTAn"
   },
   "source": [
    "# Preprocessing and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uo-6AY5QhS0A"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S8yAHw8ysf1r"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Removing Punctuation\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "    # Removing Stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Joining the tokens back into a single string\n",
    "    processed_text = ' '.join(tokens)\n",
    "\n",
    "    return processed_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BfhVJlnUsrmm"
   },
   "outputs": [],
   "source": [
    "# Apply preprocessing to the 'text' column of the dataset\n",
    "df['text'] = df['text'].apply(preprocess_text)\n",
    "# Print the preprocessed text\n",
    "print(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WBby2pPhs4v2"
   },
   "outputs": [],
   "source": [
    "#view sample of data after cleaning and preprocessing\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Fxip0GjxxZ7"
   },
   "source": [
    "# Bag of Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MVmm-MBavFRq"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create an object used to transform text data into a matrix of token counts.\n",
    "bow_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HAA5dKx8vIqP"
   },
   "outputs": [],
   "source": [
    "# Apply bag of words on the preprocessed text column\n",
    "X_bow = bow_vectorizer.fit_transform(df['text'])\n",
    "\n",
    "# Print the bag of words matrix\n",
    "print(X_bow.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWhOD30Ix5Hc"
   },
   "source": [
    "# Bag of Ngrams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FAeFL6yvt3VT"
   },
   "outputs": [],
   "source": [
    "#breaking down a text into sequences of  words of length n and counting the frequency of each sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kx4U3pU8vToK"
   },
   "outputs": [],
   "source": [
    "# Create an instance of the CountVectorizer for bag of n-grams\n",
    "ngram_vectorizer = CountVectorizer(ngram_range=(2, 3))\n",
    "#considers all  sequences of 2 and 3 words in the text and counts their occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ELlvxVhvT3K"
   },
   "outputs": [],
   "source": [
    "# Apply bag of n-grams on the preprocessed text column\n",
    "X_ngram = ngram_vectorizer.fit_transform(df['text'])\n",
    "\n",
    "# Print the bag of n-grams matrix\n",
    "print(X_ngram.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cp6gQkaFyC3i"
   },
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4nJXl9tRuPBd"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create an instance of the TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EIPNIy_aurLZ"
   },
   "outputs": [],
   "source": [
    "# Apply TF-IDF on the preprocessed text column\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df['text'])\n",
    "\n",
    "# Print the TF-IDF matrix\n",
    "print(X_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCYRcRV2yIbK"
   },
   "source": [
    "# OneHotEncoder & LabelEncoder for Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ys9r4UKPDym"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "# Create an instance of LabelEncoder for label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Apply label encoding on the preprocessed text column\n",
    "labels = label_encoder.fit_transform(df['text'])\n",
    "\n",
    "# Print the encoded labels\n",
    "print(labels)\n",
    "\n",
    "# Reshape the labels to a column vector\n",
    "labels = labels.reshape(-1, 1)\n",
    "\n",
    "# Create an instance of OneHotEncoder for one-hot encoding\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Apply one-hot encoding on the labels\n",
    "onehot_labels = onehot_encoder.fit_transform(labels)\n",
    "\n",
    "# Print the one-hot encoded labels\n",
    "print(onehot_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UhGl-SGrDopB"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# # Create an object used to transform text data into a matrix of token counts.\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "07GKCezbDt5K"
   },
   "outputs": [],
   "source": [
    "# Preprocess the text data and apply CountVectorizer\n",
    "text_data = df['text']\n",
    "text_features = vectorizer.fit_transform(text_data.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MFJ9mAnBEMxD"
   },
   "source": [
    "#Label Encoder and Full Sample target variable and independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uv6gBMtspwGc"
   },
   "outputs": [],
   "source": [
    "# Convert the labels to numerical values like 'red', 'green', and 'blue', label encoding would transform them into numerical values like 0, 1, and 2.\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(df['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQo8gd05iaOx"
   },
   "source": [
    "# Splitting to training and testing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1dX2vav7D8RT"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_features, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZiFQS2cUilz8"
   },
   "source": [
    "#Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-jmRetNMZ5b4"
   },
   "outputs": [],
   "source": [
    "#statistical method used for binary classification,output is transformed using the logistic function (sigmoid) to produce values between 0 and 1\n",
    "# goal is to predict a binary outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZfKEdzYD9mSm"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create an instance of the logistic regression model\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "rounded_acc_lr = model.score(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Accuracy:\", rounded_acc_lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CekW2RXdErwm"
   },
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PExtZ1pMZtMA"
   },
   "outputs": [],
   "source": [
    "#XGBoost Model is correcting errors made by previous models and improve predictive accuracy and speed by sequentially adding weak learners (decision trees)\n",
    "#used for both regression and classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "om_JYiiVDVBO"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Create an instance of the Gradient Boosting classifier\n",
    "model = GradientBoostingClassifier()\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "rounded_acc_xgb  = model.score(X_test, y_test)\n",
    "\n",
    "\n",
    "print(\"Accuracy:\", rounded_acc_xgb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QsJED-uCZkrA"
   },
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AjRvbyQpZ3Ep"
   },
   "outputs": [],
   "source": [
    "#Random Forest model is builds multiple decision trees during training and combines their predictions to improve accuracy and generalization.\n",
    "#provide robust predictions for both classification and regression tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l5beM6I0XbyU"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create an instance of the Random Forest classifier\n",
    "model_rf = RandomForestClassifier()\n",
    "\n",
    "# Fit the RandomForest model on the training data\n",
    "model_rf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the RandomForest model on the test data\n",
    "rounded_acc_rf  = model_rf.score(X_test, y_test)\n",
    "\n",
    "\n",
    "print(\"Accuracy:\", rounded_acc_rf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JBrK8TWbnou"
   },
   "source": [
    "#Chi-Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jD48enpCbu7h"
   },
   "outputs": [],
   "source": [
    "#Chi-Square statistic measures the independence  on categorical variables\n",
    "#association between each feature and the target variable in classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g_eX4SNTaq5K"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "# Apply Chi-Square feature selection\n",
    "k_best = SelectKBest(chi2, k=1000)  # Adjust the value of k based on your requirements\n",
    "X_train_chi2 = k_best.fit_transform(X_train, y_train)\n",
    "X_test_chi2 = k_best.transform(X_test)\n",
    "\n",
    "# Create an instance of the RandomForest classifier for Chi-Square model\n",
    "model_chi2 = RandomForestClassifier()\n",
    "\n",
    "# Fit the RandomForest model with Chi-Square features on the training data\n",
    "model_chi2.fit(X_train_chi2, y_train)\n",
    "\n",
    "# Evaluate the RandomForest model with Chi-Square features on the test data\n",
    "rounded_acc_chi2 = model_chi2.score(X_test_chi2, y_test)\n",
    "\n",
    "print(\"Accuracy (Chi-Square):\", rounded_acc_chi2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GpeOC88ikjaK"
   },
   "source": [
    "#Models Compared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wNPj8vyfHpIU"
   },
   "outputs": [],
   "source": [
    "models = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'XGBoost', 'Random Forest','Chi-Square'],\n",
    "    'R-squared Score': [ rounded_acc_lr*100, rounded_acc_xgb*100,rounded_acc_rf*100,rounded_acc_chi2*100]})\n",
    "models.sort_values(by='R-squared Score', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
